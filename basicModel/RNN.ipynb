{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session-based Recs with Transformers4Rec: RNN - Gated Recurrent Networks\n",
    "\n",
    "Followed a step by step tutorial:\n",
    "https://nvidia-merlin.github.io/Transformers4Rec/main/examples/tutorial/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers4rec import tf as tr\n",
    "import tensorflow as tf\n",
    "from transformers4rec.tf.ranking_metric import NDCGAt, RecallAt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiates Schema object from schema file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", '../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin_standard_lib import Schema\n",
    "# define schema object to pass it to the TabularSeqeunceFeatures class\n",
    "SCHEMA_PATH = os.path.join(INPUT_DATA_DIR, 'schema.pb')\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "schema = schema.select_by_name(['product_id-list_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"price_log_norm-list_seq\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 20\n",
      "  }\n",
      "  type: FLOAT\n",
      "  float_domain {\n",
      "    name: \"price_log_norm-list_seq\"\n",
      "    min: -17.176351827798428\n",
      "    max: 1.7566816406751988\n",
      "  }\n",
      "  annotation {\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"product_recency_days_log_norm-list_seq\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 20\n",
      "  }\n",
      "  type: FLOAT\n",
      "  float_domain {\n",
      "    name: \"product_recency_days_log_norm-list_seq\"\n",
      "    min: -6.913329620541532\n",
      "    max: 0.44860732556877836\n",
      "  }\n",
      "  annotation {\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# inspect the first lines of schema.pb\n",
    "!head -30 $SCHEMA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the input block: `TabularSequenceFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 13:55:15.908331: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 20\n",
    "inputs = tr.TabularSequenceFeatures.from_schema(\n",
    "    schema,\n",
    "    max_sequence_length = sequence_length,\n",
    "    masking = 'causal',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the blocks with `SequentialBlock`\n",
    "when using tensorflow inplace of pytorch means replace block with a one layer sequential block as block has no constructor in tf but does in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 128\n",
    "body = tr.SequentialBlock(\n",
    "    [inputs,\n",
    "    tr.MLPBlock([d_model]),\n",
    "    tf.keras.layers.GRU(units=d_model)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Prediction head and tying embeddings\n",
    "hf_format = True argument removed because it is not a keyword argument recognised by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = tr.Head(\n",
    "    body,\n",
    "    tr.NextItemPredictionTask(weight_tying=True,\n",
    "                              metrics=[NDCGAt(top_ks=[10, 20], labels_onehot=True),\n",
    "                                       RecallAt(top_ks=[10, 20], labels_onehot=True)]),\n",
    ")\n",
    "model = tr.Model(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***disregard the dataloader function from schema used in tutorial as this is used in the transformers4rec.torch trainer class which doesn't exist for tf***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Fine-Tuning: Training over a time window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers4rec.config.trainer import T4RecTrainingArguments\n",
    "\n",
    "#Set arguments for training\n",
    "train_args = T4RecTrainingArguments(local_rank = -1,\n",
    "                                    dataloader_drop_last = False,\n",
    "                                    report_to = [],   #set empy list to avoig logging metrics to Weights&Biases\n",
    "                                    gradient_accumulation_steps = 1,\n",
    "                                    per_device_train_batch_size = 256,\n",
    "                                    per_device_eval_batch_size = 32,\n",
    "                                    output_dir = \"./tmp\",\n",
    "                                    max_sequence_length=sequence_length,\n",
    "                                    learning_rate=0.00071,\n",
    "                                    num_train_epochs=3,\n",
    "                                    logging_steps=200,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Trainer\n",
    "doesn't include schema argument as it is not an accepted argument for HF Trainer Class compared to the tr Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/trainer_tf.py:109: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/master/examples/tensorflow\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFTrainer\n",
    "\n",
    "# Instantiate the T4Rec Trainer, which manages training and evaluation\n",
    "trainer = TFTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    compute_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output of the processed parquet files\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"../data/sessions_by_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
