{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session-based Recs with Transformers4Rec: RNN - Gated Recurrent Networks\n",
    "\n",
    "Followed a step by step tutorial:\n",
    "https://nvidia-merlin.github.io/Transformers4Rec/main/examples/tutorial/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers4rec import tf as tr\n",
    "import tensorflow as tf\n",
    "from transformers4rec.tf.ranking_metric import NDCGAt, RecallAt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiates Schema object from schema file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input file path\n",
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", '../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output file path\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"../data/sessions_by_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features chosen to train on\n",
    "chosen_features = ['product_id-list_seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin_standard_lib import Schema\n",
    "# define schema object to pass it to the TabularSeqeunceFeatures class\n",
    "SCHEMA_PATH = os.path.join(INPUT_DATA_DIR, 'schema.pb')\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "schema = schema.select_by_name(chosen_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"price_log_norm-list_seq\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 20\n",
      "  }\n",
      "  type: FLOAT\n",
      "  float_domain {\n",
      "    name: \"price_log_norm-list_seq\"\n",
      "    min: -17.176351827798428\n",
      "    max: 1.7566816406751988\n",
      "  }\n",
      "  annotation {\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"product_recency_days_log_norm-list_seq\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 20\n",
      "  }\n",
      "  type: FLOAT\n",
      "  float_domain {\n",
      "    name: \"product_recency_days_log_norm-list_seq\"\n",
      "    min: -6.913329620541532\n",
      "    max: 0.44860732556877836\n",
      "  }\n",
      "  annotation {\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# inspect the first lines of schema.pb\n",
    "!head -30 $SCHEMA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the input block: `TabularSequenceFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 12:10:13.826547: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 20\n",
    "inputs = tr.TabularSequenceFeatures.from_schema(\n",
    "    schema,\n",
    "    max_sequence_length = sequence_length,\n",
    "    masking = 'causal'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the blocks with `SequentialBlock`\n",
    "when using tensorflow inplace of pytorch means replace block with a one layer sequential block as block has no constructor in tf but does in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 128\n",
    "body = tr.SequentialBlock(\n",
    "    [inputs,\n",
    "    tr.MLPBlock([d_model]),\n",
    "    tf.keras.layers.GRU(units=d_model,return_sequences=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialBlock(\n",
       "  (layers): List(\n",
       "    (0): TabularSequenceFeatures(\n",
       "      (to_merge): Dict(\n",
       "        (categorical_layer): SequenceEmbeddingFeatures(\n",
       "          (feature_config): Dict(\n",
       "            (product_id-list_seq): TableConfig(vocabulary_size=118335, dim=64, initializer=None, optimizer=None, combiner='mean', name='product_id-list_seq')\n",
       "          )\n",
       "          (_pre): SequentialTabularTransformations(\n",
       "            (layers): List(\n",
       "              (0): FilterFeatures(\n",
       "                (to_include): List(\n",
       "                  (0): 'product_id-list_seq'\n",
       "                )\n",
       "              )\n",
       "              (1): AsSparseFeatures()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (_aggregation): ConcatFeatures()\n",
       "      (_masking): CausalLanguageModeling()\n",
       "    )\n",
       "    (1): MLPBlock(\n",
       "      (layers): List(\n",
       "        (0): Dense(128, activation=relu, use_bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): GRU(\n",
       "      (cell): GRUCell()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Prediction head and tying embeddings\n",
    "hf_format = True argument removed because it is not a keyword argument recognised by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head = tr.Head(\n",
    "#     body,\n",
    "#     tr.NextItemPredictionTask(weight_tying=True,\n",
    "#                               metrics=[NDCGAt(top_ks=[10, 20], labels_onehot=True),\n",
    "#                                        RecallAt(top_ks=[10, 20], labels_onehot=True)]),\n",
    "# )\n",
    "# model = tr.Model(head)\n",
    "head = tr.Head(\n",
    "    body,\n",
    "    tr.NextItemPredictionTask(weight_tying=True,\n",
    "                              metrics=[tf.keras.metrics.AUC])\n",
    ")\n",
    "model = tr.Model(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss = \"binary_crossentropy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***disregard the dataloader function from schema used in tutorial as this is used in the transformers4rec.torch trainer class which doesn't exist for tf***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_over_df(\n",
    "    ### iterator function as input for the tensorflow generator `from_generator` function\n",
    "    df: pd.DataFrame\n",
    "):  \n",
    "    def caller():\n",
    "        for _,j in df.iterrows():\n",
    "            yield(j['product_id-list_seq'])\n",
    "    return caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_from_df(\n",
    "    ### generate tensorflow object from dataframe\n",
    "    df: pd.DataFrame\n",
    "):\n",
    "    output_shape_x = (\n",
    "        tf.TensorShape([None,])\n",
    "    )\n",
    "    df = tf.data.Dataset.from_generator(\n",
    "        iterate_over_df(df),\n",
    "        output_types=(tf.int32),\n",
    "        output_shapes = (output_shape_x)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_dataset(\n",
    "        ## pad dataset so all session sequence data have length 20\n",
    "        df,\n",
    "        batch_size: int,\n",
    "):\n",
    "    df = df.shuffle(5)\n",
    "    df = df.padded_batch(batch_size, padded_shapes = (([20,])), padding_values = ((0)),drop_remainder=True)\n",
    "    df = df.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_dict(\n",
    "    ### create a dictionary tensor dataframe as input into the model\n",
    "    df_list: list,\n",
    "    chosen_features: list\n",
    "):\n",
    "    df_dictionary = {}\n",
    "    if len(chosen_features) == 1:\n",
    "        df_dictionary[chosen_features[0]] = df_list[0]\n",
    "    else:\n",
    "        for i in range(len(df_list[0])):\n",
    "            df_dictionary[chosen_features[i]] = df_list[0][i]\n",
    "    return df_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    ### function to call all other functions necessary to build the dataset\n",
    "    ### to input into the model\n",
    "    df,\n",
    "    batch_size,\n",
    "    df_len\n",
    "):\n",
    "    df = ds_from_df(df)\n",
    "    df = pad_dataset(df,df_len)\n",
    "    df = data_to_dict(list(df),chosen_features)\n",
    "    targets = {\"target\": tf.cast(tf.random.uniform((df_len,), maxval=2, dtype=tf.int32), tf.float32)}\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df, targets)).batch(50)\n",
    "    steps = int(np.floor(df_len/batch_size))\n",
    "\n",
    "    return ds, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fine-tuning and Incremental evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 256\n",
    "eval_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training for day 1 are:\n",
      "********************\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projecting inputs of NextItemPredictionTask to'64' As weight tying requires the input dimension '128' to be equal to the item-id embedding dimension '64'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 456, in train_step\n        loss = self.compute_loss(inputs, targets, training=True)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 524, in compute_loss\n        [\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 525, in <listcomp>\n        head.compute_loss(\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 396, in compute_loss\n        loss = task.compute_loss(\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/prediction_task.py\", line 250, in compute_loss\n        update_ops = self.calculate_metrics(predictions, targets, forward=False, loss=loss)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/prediction_task.py\", line 279, in calculate_metrics\n        metric.update_state(y_true=targets, y_pred=predictions, sample_weight=sample_weight)\n    File \"/usr/local/lib/python3.9/site-packages/keras/utils/metrics_utils.py\", line 73, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/site-packages/keras/metrics.py\", line 177, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/site-packages/keras/metrics.py\", line 2343, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"/usr/local/lib/python3.9/site-packages/keras/utils/metrics_utils.py\", line 625, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 118335) and (None,) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 456, in train_step\n        loss = self.compute_loss(inputs, targets, training=True)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 524, in compute_loss\n        [\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 525, in <listcomp>\n        head.compute_loss(\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 396, in compute_loss\n        loss = task.compute_loss(\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/prediction_task.py\", line 250, in compute_loss\n        update_ops = self.calculate_metrics(predictions, targets, forward=False, loss=loss)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/prediction_task.py\", line 279, in calculate_metrics\n        metric.update_state(y_true=targets, y_pred=predictions, sample_weight=sample_weight)\n    File \"/usr/local/lib/python3.9/site-packages/keras/utils/metrics_utils.py\", line 73, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/site-packages/keras/metrics.py\", line 177, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/site-packages/keras/metrics.py\", line 2343, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"/usr/local/lib/python3.9/site-packages/keras/utils/metrics_utils.py\", line 625, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 118335) and (None,) are incompatible\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# window\n",
    "start_time_window_index = 1\n",
    "final_time_window_index = 4\n",
    "for time_index in range(start_time_window_index, final_time_window_index):\n",
    "    # Set data\n",
    "\n",
    "    time_index_train = time_index\n",
    "    time_index_eval = time_index + 1\n",
    "    train_paths = os.path.join(OUTPUT_DIR, f\"{time_index_train}/train.parquet\")\n",
    "    eval_paths = os.path.join(OUTPUT_DIR, f\"{time_index_eval}/valid.parquet\")\n",
    "\n",
    "    # Initialize dataloaders\n",
    "    train_df = pd.read_parquet(train_paths)\n",
    "    train_df = train_df[['product_id-list_seq']]\n",
    "    eval_df = pd.read_parquet(eval_paths)\n",
    "    eval_df = eval_df[['product_id-list_seq']]\n",
    "\n",
    "    # Find length of dataframes for argument into `get_dataset`\n",
    "    train_len = len(train_df)\n",
    "    eval_len = len(eval_df)\n",
    "\n",
    "    # get datasets\n",
    "\n",
    "    train_dataset, train_steps = get_dataset(train_df, train_batch_size,train_len)\n",
    "    eval_dataset, eval_steps = get_dataset(eval_df, eval_batch_size,eval_len)\n",
    "\n",
    "    # Train on day related to time_index \n",
    "    # Launch Training\n",
    "    print('*'*20)\n",
    "    print(\"Launch training for day %s are:\" %time_index)\n",
    "    print('*'*20 + '\\n')\n",
    "    \n",
    "    n_epochs = 3\n",
    "    history = model.fit(\n",
    "        train_dataset.repeat(n_epochs),\n",
    "        steps_per_epoch=train_steps,\n",
    "        epochs=n_epochs,\n",
    "        initial_epoch=0,\n",
    "        verbose=1,\n",
    "    )\n",
    "    # Launch Eval\n",
    "    print('*'*20)\n",
    "    print(\"Launch evaluation for day %s are:\" %time_index)\n",
    "    print('*'*20 + '\\n')\n",
    "    results = model.evaluate(eval_dataset, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
