{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session-based Recs with Transformers4Rec: RNN - Gated Recurrent Networks\n",
    "\n",
    "Followed a step by step tutorial:\n",
    "https://nvidia-merlin.github.io/Transformers4Rec/main/examples/tutorial/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers4rec import tf as tr\n",
    "import tensorflow as tf\n",
    "from transformers4rec.tf.ranking_metric import NDCGAt, RecallAt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiates Schema object from schema file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", '../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin_standard_lib import Schema\n",
    "# define schema object to pass it to the TabularSeqeunceFeatures class\n",
    "SCHEMA_PATH = os.path.join(INPUT_DATA_DIR, 'schema.pb')\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "schema = schema.select_by_name(['product_id-list_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"price_log_norm-list_seq\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 20\n",
      "  }\n",
      "  type: FLOAT\n",
      "  float_domain {\n",
      "    name: \"price_log_norm-list_seq\"\n",
      "    min: -17.176351827798428\n",
      "    max: 1.7566816406751988\n",
      "  }\n",
      "  annotation {\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"product_recency_days_log_norm-list_seq\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 20\n",
      "  }\n",
      "  type: FLOAT\n",
      "  float_domain {\n",
      "    name: \"product_recency_days_log_norm-list_seq\"\n",
      "    min: -6.913329620541532\n",
      "    max: 0.44860732556877836\n",
      "  }\n",
      "  annotation {\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# inspect the first lines of schema.pb\n",
    "!head -30 $SCHEMA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the input block: `TabularSequenceFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 16:57:26.390310: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 20\n",
    "inputs = tr.TabularSequenceFeatures.from_schema(\n",
    "    schema,\n",
    "    max_sequence_length = sequence_length,\n",
    "    masking = 'causal',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the blocks with `SequentialBlock`\n",
    "when using tensorflow inplace of pytorch means replace block with a one layer sequential block as block has no constructor in tf but does in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 128\n",
    "body = tr.SequentialBlock(\n",
    "    [inputs,\n",
    "    tr.MLPBlock([d_model]),\n",
    "    tf.keras.layers.GRU(units=d_model)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Prediction head and tying embeddings\n",
    "hf_format = True argument removed because it is not a keyword argument recognised by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head = tr.Head(\n",
    "#     body,\n",
    "#     tr.NextItemPredictionTask(weight_tying=True,\n",
    "#                               metrics=[NDCGAt(top_ks=[10, 20], labels_onehot=True),\n",
    "#                                        RecallAt(top_ks=[10, 20], labels_onehot=True)]),\n",
    "# )\n",
    "# model = tr.Model(head)\n",
    "head = tr.Head(\n",
    "    body,\n",
    "    tr.NextItemPredictionTask(weight_tying=True,\n",
    "                              metrics=[tf.keras.metrics.AUC])\n",
    ")\n",
    "model = tr.Model(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***disregard the dataloader function from schema used in tutorial as this is used in the transformers4rec.torch trainer class which doesn't exist for tf***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Fine-Tuning: Training over a time window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output of the processed parquet files\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"../data/sessions_by_day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fine-tuning and Incremental evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randint\n",
    "def iterate_over_df(\n",
    "    df: pd.DataFrame\n",
    "):  \n",
    "    seed(1)\n",
    "    y_value = randint(0, 10)\n",
    "    def caller():\n",
    "        for _,j in df.iterrows():\n",
    "            yield(j['user_session'],\n",
    "            j['product_id-list_seq']), y_value\n",
    "    return caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_from_df(\n",
    "    df: pd.DataFrame\n",
    "):\n",
    "    output_shape_x = (\n",
    "        tf.TensorShape([]),\n",
    "        tf.TensorShape([None, ])\n",
    "    )\n",
    "    df = tf.data.Dataset.from_generator(\n",
    "        iterate_over_df(df),\n",
    "        output_types=((tf.int32, tf.int32), tf.int32),\n",
    "        output_shapes = (output_shape_x, tf.TensorShape([]))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_dataset(\n",
    "        df,\n",
    "        batch_size: int,\n",
    "        # gender: str,\n",
    "        # z='all'\n",
    "):\n",
    "    df = df.shuffle(5)\n",
    "    # datset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    df = df.padded_batch(batch_size, padded_shapes = (([],[20,]),[]), padding_values = ((0, 0),0),drop_remainder=True)\n",
    "    df = df.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(        \n",
    "    df: pd.DataFrame,\n",
    "    # candidates,\n",
    "    # product_id_to_type: pd.Series,\n",
    "    batch_size: int,\n",
    "    # gender: str,\n",
    "    # show_history: bool,\n",
    "    # z='all'\n",
    "):\n",
    "    df = ds_from_df(df)\n",
    "    df = batch_dataset(df,batch_size)\n",
    "    df = df.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_or_test(\n",
    "        df: pd.DataFrame,\n",
    "        batch_size: int\n",
    "        # candidates,\n",
    "        # product_id_to_type: pd.Series,\n",
    "        # gender: str,\n",
    "        # show_history: bool,\n",
    "        # z='all'\n",
    "):\n",
    "    # calculate steps relative to batch size\n",
    "    steps = int(np.floor(df.shape[0] / batch_size))\n",
    "    # remove incomplete batches\n",
    "    df = df.iloc[:steps * batch_size]\n",
    "    df = get_dataset(\n",
    "        df,\n",
    "        # candidates=candidates,\n",
    "        # product_id_to_type=product_id_to_type,\n",
    "        batch_size=batch_size\n",
    "        # gender=gender,\n",
    "        # show_history=show_history,\n",
    "        # z=z\n",
    "    )\n",
    "    return df, steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 256\n",
    "eval_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 495 ms, sys: 66.2 ms, total: 562 ms\n",
      "Wall time: 452 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# start_time_window_index = 1\n",
    "# final_time_window_index = 4\n",
    "# for time_index in range(start_time_window_index, final_time_window_index):\n",
    "#     # Set data\n",
    "time_index = 1\n",
    "time_index_train = time_index\n",
    "time_index_eval = time_index + 1\n",
    "train_paths = os.path.join(OUTPUT_DIR, f\"{time_index_train}/train.parquet\")\n",
    "eval_paths = os.path.join(OUTPUT_DIR, f\"{time_index_eval}/valid.parquet\")\n",
    "\n",
    "# Initialize dataloaders\n",
    "train_df = pd.read_parquet(train_paths)\n",
    "train_df = train_df[['user_session', 'product_id-list_seq']]\n",
    "eval_df = pd.read_parquet(eval_paths)\n",
    "eval_df = eval_df[['user_session', 'product_id-list_seq']]\n",
    "train_dataset, train_steps = get_train_or_test(train_df, train_batch_size)\n",
    "eval_dataset, eval_steps = get_train_or_test(eval_df, eval_batch_size)\n",
    "\n",
    "# # Train on day related to time_index\n",
    "# print('*'*20)\n",
    "# print(\"Launch training for day %s are:\" %time_index)\n",
    "# print('*'*20 + '\\n')\n",
    "\n",
    "# n_epochs = 3\n",
    "# history = model.fit(\n",
    "#     train_dataset.repeat(n_epochs),\n",
    "#     steps_per_epoch=train_steps,\n",
    "#     epochs=n_epochs,\n",
    "#     initial_epoch=0,  # , callbacks=[pc],\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training for day 1 are:\n",
      "********************\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 456, in train_step\n        loss = self.compute_loss(inputs, targets, training=True)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 524, in compute_loss\n        [\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 525, in <listcomp>\n        head.compute_loss(\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 392, in compute_loss\n        body_outputs = self.body(body_outputs)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/config/schema.py\", line 50, in __call__\n        return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/block/base.py\", line 127, in build\n        input_shape = layer.compute_output_shape(input_shape)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/features/sequence.py\", line 281, in compute_output_shape\n        output_shapes = super().compute_output_shape(input_shapes)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/tabular/base.py\", line 398, in compute_output_shape\n        output_shapes = self._check_post_output_size(self.compute_call_output_shape(input_shapes))\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/features/sequence.py\", line 276, in compute_call_output_shape\n        output_shapes.update(layer.compute_output_shape(input_shape))\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/tabular/base.py\", line 396, in compute_output_shape\n        input_shapes = self.pre.compute_output_shape(input_shapes)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/block/base.py\", line 103, in compute_output_shape\n        output_shape = layer.compute_output_shape(output_shape)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/tabular/base.py\", line 564, in compute_output_shape\n        return {k: v for k, v in input_shape.items() if k in self.to_include}\n\n    AttributeError: 'tuple' object has no attribute 'items'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4j/6q5w3p6s7zb16_63n6np2z1c0000gr/T/ipykernel_53473/1705867650.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 456, in train_step\n        loss = self.compute_loss(inputs, targets, training=True)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 524, in compute_loss\n        [\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 525, in <listcomp>\n        head.compute_loss(\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/model/base.py\", line 392, in compute_loss\n        body_outputs = self.body(body_outputs)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/config/schema.py\", line 50, in __call__\n        return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/block/base.py\", line 127, in build\n        input_shape = layer.compute_output_shape(input_shape)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/features/sequence.py\", line 281, in compute_output_shape\n        output_shapes = super().compute_output_shape(input_shapes)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/tabular/base.py\", line 398, in compute_output_shape\n        output_shapes = self._check_post_output_size(self.compute_call_output_shape(input_shapes))\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/features/sequence.py\", line 276, in compute_call_output_shape\n        output_shapes.update(layer.compute_output_shape(input_shape))\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/tabular/base.py\", line 396, in compute_output_shape\n        input_shapes = self.pre.compute_output_shape(input_shapes)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/block/base.py\", line 103, in compute_output_shape\n        output_shape = layer.compute_output_shape(output_shape)\n    File \"/usr/local/lib/python3.9/site-packages/transformers4rec/tf/tabular/base.py\", line 564, in compute_output_shape\n        return {k: v for k, v in input_shape.items() if k in self.to_include}\n\n    AttributeError: 'tuple' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "# Train on day related to time_index\n",
    "print('*'*20)\n",
    "print(\"Launch training for day %s are:\" %time_index)\n",
    "print('*'*20 + '\\n')\n",
    "\n",
    "n_epochs = 3\n",
    "history = model.fit(\n",
    "    train_dataset.repeat(n_epochs),\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=n_epochs,\n",
    "    initial_epoch=0,  # , callbacks=[pc],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256,), dtype=int32, numpy=\n",
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32)>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
