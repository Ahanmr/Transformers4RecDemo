{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session-based Recs with Transformers4Rec: RNN - Gated Recurrent Networks\n",
    "\n",
    "Followed a step by step tutorial:\n",
    "https://nvidia-merlin.github.io/Transformers4Rec/main/examples/tutorial/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers4rec import tf as tr\n",
    "import tensorflow as tf\n",
    "from transformers4rec.tf.ranking_metric import NDCGAt, RecallAt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiates Schema object from schema file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input file path\n",
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", '../data/')\n",
    "# define the output file path\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"../data/sessions_by_day\")\n",
    "# features chosen to train on\n",
    "chosen_features = ['product_id-list_seq']\n",
    "from merlin_standard_lib import Schema\n",
    "# define schema object to pass it to the TabularSeqeunceFeatures class\n",
    "SCHEMA_PATH = os.path.join(INPUT_DATA_DIR, 'schema.pb')\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "schema = schema.select_by_name(chosen_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Input Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use MLM as the training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "sequence_length, d_model = 20, 192\n",
    "# Define input module to process tabular input-features and to prepare masked inputs\n",
    "inputs = tr.TabularSequenceFeatures.from_schema(\n",
    "    schema,\n",
    "    max_sequence_length = sequence_length,\n",
    "    d_output = d_model,\n",
    "    masking = 'mlm'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define XLNetConfig class and set default parameters for HF XLNet config\n",
    "transformer_config = tr.XLNetConfig.build(\n",
    "    d_model = d_model, n_head=4, n_layer=2, total_seq_length=sequence_length\n",
    ")\n",
    "\n",
    "# define the model block including: inputs, masking, projection and transformer block.\n",
    "\n",
    "body = tr.SequentialBlock(\n",
    "    [inputs,\n",
    "    tr.MLPBlock([192]),\n",
    "    tr.TransformerBlock(transformer_config, masking=inputs.masking)]\n",
    ")\n",
    "\n",
    "# define the head for to the next item prediction task\n",
    "\n",
    "head = tr.Head(\n",
    "    body,\n",
    "    tr.NextItemPredictionTask(\n",
    "        weight_tying=True, \n",
    "        # hf_format=True, \n",
    "        metrics=[NDCGAt(top_ks=[10, 20], labels_onehot=True),RecallAt(top_ks=[10, 20], labels_onehot=True)],\n",
    "        # loss = tf.keras.losses.CategoricalCrossentropy)\n",
    "))\n",
    "# head = tr.Head(\n",
    "#     body,\n",
    "#     tr.NextItemPredictionTask(\n",
    "#         weight_tying=True, \n",
    "#         # hf_format=True, \n",
    "#         metrics=[NDCGAt_temp(top_ks=[10, 20], labels_onehot=False)]\n",
    "#         # loss = tf.keras.losses.CategoricalCrossentropy)\n",
    "# ))\n",
    "\n",
    "# get the end-to-end Model class\n",
    "\n",
    "model = tr.Model(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_over_df(\n",
    "    ### iterator function as input for the tensorflow generator `from_generator` function\n",
    "    df: pd.DataFrame\n",
    "):  \n",
    "    df['empty_list'] = [[] for _ in range(len(df))]\n",
    "    def caller():\n",
    "        for _,j in df.iterrows():\n",
    "            yield(j['product_id-list_seq']),j['empty_list']\n",
    "    return caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_from_df(\n",
    "    ### generate tensorflow object from dataframe\n",
    "    df: pd.DataFrame\n",
    "):\n",
    "    output_shape_x = (\n",
    "        tf.TensorShape([None,])\n",
    "    )\n",
    "    df = tf.data.Dataset.from_generator(\n",
    "        iterate_over_df(df),\n",
    "        output_types=((tf.int32),tf.int32),\n",
    "        output_shapes = (output_shape_x, tf.TensorShape([None,]))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_dataset(\n",
    "        ## pad dataset so all session sequence data have length 20\n",
    "        df,\n",
    "        batch_size: int,\n",
    "):\n",
    "    df = df.padded_batch(batch_size, padded_shapes = (([20,]),[0,]), padding_values = ((0),0),drop_remainder=True)\n",
    "    df = df.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_dict(\n",
    "    ### create a dictionary tensor dataframe as input into the model\n",
    "    df_list: list,\n",
    "    chosen_features: list\n",
    "):\n",
    "    df_dictionary = {}\n",
    "    if len(chosen_features) == 1:\n",
    "        df_dictionary[chosen_features[0]] = df_list[0][0]\n",
    "    else:\n",
    "        for i in range(len(df_list[0]),-1):\n",
    "            df_dictionary[chosen_features[i]] = df_list[0][i]\n",
    "    return (df_dictionary,df_list[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    ### function to call all other functions necessary to build the dataset\n",
    "    ### to input into the model\n",
    "    df,\n",
    "    batch_size,\n",
    "    df_len\n",
    "):\n",
    "    df = ds_from_df(df)\n",
    "    df = pad_dataset(df,df_len)\n",
    "    df = data_to_dict(list(df),chosen_features)\n",
    "    # targets = {\"target\": tf.cast(tf.random.uniform((df_len,), maxval=2, dtype=tf.int32), tf.float32)}\n",
    "    # targets = df[\"product_id-list_seq\"]\n",
    "    ds = tf.data.Dataset.from_tensor_slices(df).batch(batch_size)\n",
    "    print(list(ds))\n",
    "    steps = int(np.floor(df_len/batch_size))\n",
    "\n",
    "    return ds, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Fine-tuning: Training over a time window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 256\n",
    "eval_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# window\n",
    "start_time_window_index = 1\n",
    "final_time_window_index = 3\n",
    "# Iterating over days of one week\n",
    "for time_index in range(start_time_window_index, final_time_window_index):\n",
    "    # Set data \n",
    "    time_index_train = time_index\n",
    "    time_index_eval = time_index + 1\n",
    "    train_paths = os.path.join(OUTPUT_DIR, f\"{time_index_train}/train.parquet\")\n",
    "    eval_paths = os.path.join(OUTPUT_DIR, f\"{time_index_eval}/valid.parquet\")\n",
    "\n",
    "    # Load data\n",
    "    train_df = pd.read_parquet(train_paths)\n",
    "    train_df = train_df[['product_id-list_seq']]\n",
    "    eval_df = pd.read_parquet(eval_paths)\n",
    "    eval_df = eval_df[['product_id-list_seq']]\n",
    "\n",
    "    # find length of dataframes for argument into `get_dataset`\n",
    "    train_len = len(train_df)\n",
    "    eval_len = len(eval_df)\n",
    "\n",
    "    # get datasets\n",
    "    train_dataset, train_steps = get_dataset(train_df, train_batch_size,train_len)\n",
    "    eval_dataset, eval_steps = get_dataset(eval_df, eval_batch_size,eval_len)\n",
    "    \n",
    "    # Train on day related to time_index \n",
    "    print('*'*20)\n",
    "    print(\"Launch training for day %s are:\" %time_index)\n",
    "    print('*'*20 + '\\n')\n",
    "    losses = model.fit(train_dataset, epochs=5)\n",
    "    model.reset_metrics()\n",
    "    # Evaluate on the following day\n",
    "    eval_metrics = model.evaluate(eval_dataset, return_dict=True)\n",
    "    print('*'*20)\n",
    "    print(\"Eval results for day %s are:\\t\" %time_index_eval)\n",
    "    print('\\n' + '*'*20 + '\\n')\n",
    "    for key in sorted(eval_metrics.keys()):\n",
    "        print(\" %s = %s\" % (key, str(eval_metrics[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
